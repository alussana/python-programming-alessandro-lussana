{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive classificator method based on blast search needs an e-value threshold to minimize the erroneus assignments between kunitz and non-kunitz proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alessandro/Unibo/python-programming-alessandro-lussana/LB1/prj_blast_classification/docs\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion matrix\n",
    "Create a file with these columns from the blast output\n",
    "\n",
    "1 identifier\n",
    "\n",
    "2 e-value\n",
    "\n",
    "3 class (0 = negative, non-kunitz; 1 = positive, kunitz)\n",
    "\n",
    "NOTE: in the blast output:\n",
    "\n",
    "1 query id\n",
    "\n",
    "2 target id\n",
    "\n",
    "\\[...\\]\n",
    "\n",
    "11 e-value\n",
    "\n",
    "## List the minimum e-value for each sequence\n",
    "First, we need to select the minimum e-value obtained from the blast search for each sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alessandro/Unibo/python-programming-alessandro-lussana/LB1/prj_blast_classification/dataset\n",
      "sp|A0B688|PRIS_METTP\t98\n",
      "sp|A0KS65|DAPF_SHESA\t16\n",
      "sp|A0PXQ5|PAND_CLONN\t128\n",
      "sp|A0Q3B8|MURI_CLONN\t136\n",
      "sp|A0T0X2|RK3_THAPS\t62\n",
      "sp|A1ANJ2|TRPF_PELPD\t33\n",
      "sp|A1AWT1|SYFA_RUTMC\t4.8\n",
      "sp|A1CHU1|MDM34_ASPCL\t27\n",
      "sp|A1JPP1|ZAPA_YERE8\t133\n",
      "sp|A1KRG4|RL10_NEIMF\t56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided cores: 1\n",
      "Rules claiming more threads will be scaled down.\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tlist_min_evalues\n",
      "\t1\n",
      "\n",
      "rule list_min_evalues:\n",
      "    input: sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_run.gz\n",
      "    output: sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_min_eval.gz\n",
      "    jobid: 0\n",
      "    wildcards: blast_run=sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot\n",
      "\n",
      "for id in $(zcat sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_run.gz | cut -f2 | sort | uniq); do zcat sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_run.gz | grep $id | cut -f2,11 | LC_ALL=c sort -gk2 | sed -n '1p'; done | gzip > sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_min_eval.gz\n",
      "Finished job 0.\n",
      "1 of 1 steps (100%) done\n",
      "Provided cores: 1\n",
      "Rules claiming more threads will be scaled down.\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tlist_min_evalues\n",
      "\t1\n",
      "\n",
      "rule list_min_evalues:\n",
      "    input: sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_blast_run.gz\n",
      "    output: sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_blast_min_eval.gz\n",
      "    jobid: 0\n",
      "    wildcards: blast_run=sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot\n",
      "\n",
      "for id in $(zcat sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_blast_run.gz | cut -f2 | sort | uniq); do zcat sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_blast_run.gz | grep $id | cut -f2,11 | LC_ALL=c sort -gk2 | sed -n '1p'; done | gzip > sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_blast_min_eval.gz\n",
      "Finished job 0.\n",
      "1 of 1 steps (100%) done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../dataset/\n",
    "pwd\n",
    "snakemake -p sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_min_eval.gz\n",
    "snakemake -p sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_blast_min_eval.gz\n",
    "zcat sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_min_eval | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: check consistency in number of lines\n",
    "DONE: lines are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a list to feed the confusion matrix program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alessandro/Unibo/python-programming-alessandro-lussana/LB1/prj_blast_classification/dataset\n",
      "sp|A0A1Z0YU59|MAMB1_DENAN\t6.83e-15\t1\n",
      "sp|A5X2X1|VKT_SISCA\t9.60e-17\t1\n",
      "sp|A6MFL1|VKT1_DEMVE\t5.99e-15\t1\n",
      "sp|A6MFL2|VKT2_DEMVE\t6.12e-15\t1\n",
      "sp|A6MFL3|VKT3_DEMVE\t8.29e-17\t1\n",
      "sp|A6MFL4|VKT4_DEMVE\t7.86e-17\t1\n",
      "sp|A6MGX9|VKT5_DEMVE\t2.60e-15\t1\n",
      "sp|A6MGY1|VKT7_DEMVE\t8.29e-17\t1\n",
      "sp|A7X3V4|VKT1_TELDH\t7.26e-16\t1\n",
      "sp|A7X3V7|VKT1_PHIOL\t1.18e-16\t1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided cores: 1\n",
      "Rules claiming more threads will be scaled down.\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tcompute_confusion_matrix_feed_file\n",
      "\t1\n",
      "\n",
      "rule compute_confusion_matrix_feed_file:\n",
      "    input: sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_blast_min_eval.gz, sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_min_eval.gz\n",
      "    output: conf_list_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot.gz\n",
      "    jobid: 0\n",
      "    wildcards: p=sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot, n=sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot\n",
      "\n",
      "cat <(zcat sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_blast_min_eval.gz | awk '{print $0\"\t\"1}') <(zcat sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot_blast_min_eval.gz | awk '{print $0\"\t\"0}') | gzip > conf_list_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot.gz\n",
      "Finished job 0.\n",
      "1 of 1 steps (100%) done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../dataset/\n",
    "pwd\n",
    "snakemake -p conf_list_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot.gz\n",
    "zcat conf_list_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot.gz | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all the data has been generated to perform the optimization of the e-value threshold in order to make the best possible predictions. Computation of the precision matrix is performed running the following rules; the final output files will contain these info:\n",
    "* **TP**      number of true positives\n",
    "* **TN**     number of true negatives\n",
    "* **FP**      number of false positives\n",
    "* **FN**      number of false negatives\n",
    "* **acc**     accuracy\n",
    "* **tpr**     sensivity - true positive rate\n",
    "* **ppv**     precision - positive predictive value\n",
    "* **mcc**     matthews correlation coefficient\n",
    "\n",
    "TODO: Analysis of the area under the ROC should be implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alessandro/Unibo/python-programming-alessandro-lussana/LB1/prj_blast_classification/dataset\n",
      "TP\t339\n",
      "TN\t495\n",
      "FP\t5\n",
      "FN\t1\n",
      "acc\t0.992857\n",
      "tpr\t0.997059\n",
      "ppv\t0.985465\n",
      "mcc\t0.985252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Provided cores: 1\n",
      "Rules claiming more threads will be scaled down.\n",
      "Job counts:\n",
      "\tcount\tjobs\n",
      "\t1\tcompute_confusion_matrix\n",
      "\t1\n",
      "\n",
      "rule compute_confusion_matrix:\n",
      "    input: conf_list_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot.gz\n",
      "    output: conf_mat_th0.001_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot\n",
      "    jobid: 0\n",
      "    wildcards: th=0.001, p=sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot, n=sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot\n",
      "\n",
      "python ../src/buildConfMatrix.py conf_list_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot.gz 0.001 > conf_mat_th0.001_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot\n",
      "Finished job 0.\n",
      "1 of 1 steps (100%) done\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ../dataset/\n",
    "pwd\n",
    "snakemake -p conf_mat_th0.001_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot\n",
    "cat conf_mat_th0.001_of_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_pf00014_non_human_id_filter_on_swissprot_and_sprot_pf00014_human_id_filter_on_swissprot_vs_sprot_non_pf00014_global_id_sampled500_filter_on_swissprot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes for HMM classification prj\n",
    "## blastclust \n",
    "blastclust -i ids_kunitz.fasta -o ids_kunits.clust -L 0.8 -S 90\n",
    "\n",
    "clusters together the sequences that share 90% sequence identity for 0.8 sequence length(?). The output is sorted such that in the first line there is the cluster with the greatest number of sequences\n",
    "\n",
    "This will be very useful for checking and removing redundancy when considering queries in PDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM classificator framework\n",
    "* fectch pdb for kunitz and non-kunitz\n",
    "* check for redundancy and filter\n",
    "* train hmm with training set\n",
    "* optimize threshold\n",
    "* test hmm with different set (do some ten-fold cross-validation)\n",
    "* do further tests that will be discussed later\n",
    "\n",
    "### fectch pdb files\n",
    "Use RCSB PDB advanced search; note that you can filter for mutations, co-crystallization...\n",
    "\n",
    "TODO: use pdbefold to have a list of structure given a selected one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report sections\n",
    "* Datasets\n",
    "* Model Building\n",
    "* Optimization\n",
    "* Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set function in python to compare lists\n",
    "Function 'set' in python to perform intersection, union etc. Useful to compare lists of identifiers etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Endnotes\n",
    "This might be useful for software packages: http://lipid.biocomp.unibo.it/emidio/tmp/\n",
    "Song of the day: Banco de Gaia - Kincajou"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
